# -*- coding: utf-8 -*-
"""
This script implements an LSTM model for CPU usage prediction.
"""

# Import necessary libraries and modules
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from keras.models import Sequential
from keras.layers import LSTM, Dense
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import math
import warnings
import os
import sys

# Add parent directory to path to import from src
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

# Suppress warnings for cleaner output
warnings.filterwarnings('ignore')

print("Loading data for LSTM model...")

# Load the preprocessed data
# Try different locations for the data file
try:
    scaled_df = pd.read_csv('../df_scaled.csv')
    print("Loaded data from parent directory")
except FileNotFoundError:
    try:
        scaled_df = pd.read_csv('../output/df_scaled.csv')
        print("Loaded data from output directory")
    except FileNotFoundError:
        scaled_df = pd.read_csv('output/df_scaled.csv')
        print("Loaded data from local output directory")

print("Available columns:", scaled_df.columns.tolist())

# LSTM (Long Short-Term Memory) modeling

# Check if required column exists
if 'CPU usage [MHZ]' in scaled_df.columns:
    # Normalize the CPU usage data for LSTM training
    data = scaled_df['CPU usage [MHZ]'].values.reshape(-1, 1)
    scaler = MinMaxScaler(feature_range=(0, 1))
    dataset = scaler.fit_transform(data)

    # Split dataset into training and testing sets
    train_size = int(len(dataset) * 0.7)
    train, test = dataset[:train_size], dataset[train_size:]

    print(f"Training samples: {len(train)}, Testing samples: {len(test)}")

    # Convert dataset into LSTM acceptable format
    def create_dataset(dataset, look_back=1):
        X, Y = [], []
        for i in range(len(dataset)-look_back-1):
            a = dataset[i:(i+look_back), 0]
            X.append(a)
            Y.append(dataset[i + look_back, 0])
        return np.array(X), np.array(Y)

    look_back = 1
    X_train, Y_train = create_dataset(train, look_back)
    X_test, Y_test = create_dataset(test, look_back)

    # Reshape input for LSTM
    X_train = np.reshape(X_train, (X_train.shape[0], 1, X_train.shape[1]))
    X_test = np.reshape(X_test, (X_test.shape[0], 1, X_test.shape[1]))

    # Define and compile the LSTM model
    print("Creating LSTM model...")
    model = Sequential()
    model.add(LSTM(4, input_shape=(1, look_back)))
    model.add(Dense(1))
    model.compile(loss='mean_squared_error', optimizer='adam')
    
    # Train the model (limited epochs for faster execution)
    print("Training LSTM model...")
    model.fit(X_train, Y_train, epochs=5, batch_size=1, verbose=2)  # Reduced epochs for demo

    # Predict using the LSTM model
    print("Making predictions...")
    train_predict = model.predict(X_train)
    test_predict = model.predict(X_test)

    # Convert predictions back to original scale
    train_predict = scaler.inverse_transform(train_predict)
    Y_train = scaler.inverse_transform([Y_train])
    test_predict = scaler.inverse_transform(test_predict)
    Y_test = scaler.inverse_transform([Y_test])

    # Compute performance metrics
    train_mae = mean_absolute_error(Y_train[0], train_predict[:, 0])
    test_mae = mean_absolute_error(Y_test[0], test_predict[:, 0])
    train_mse = mean_squared_error(Y_train[0], train_predict[:, 0])
    test_mse = mean_squared_error(Y_test[0], test_predict[:, 0])
    train_rmse = math.sqrt(train_mse)
    test_rmse = math.sqrt(test_mse)
    train_r2 = r2_score(Y_train[0], train_predict[:, 0])
    test_r2 = r2_score(Y_test[0], test_predict[:, 0])

    # Print metrics
    print(f"Train MAE: {train_mae:.2f}")
    print(f"Test MAE: {test_mae:.2f}")
    print(f"Train MSE: {train_mse:.2f}")
    print(f"Test MSE: {test_mse:.2f}")
    print(f"Train RMSE: {train_rmse:.2f}")
    print(f"Test RMSE: {test_rmse:.2f}")
    print(f"Train R2: {train_r2:.2f}")
    print(f"Test R2: {test_r2:.2f}")

    # Plot actual vs predicted CPU usage
    plt.figure(figsize=(10, 6))
    plt.plot(Y_test[0][:50], 'b', label='Actual')  # Plot first 50 points for clarity
    plt.plot(test_predict[:50], 'g', alpha=0.7, label='Predicted')  # Plot first 50 points for clarity
    plt.title('LSTM: Actual vs Predicted CPU usage [MHZ]')
    plt.ylabel('CPU usage [MHZ]')
    plt.xlabel('Steps')
    plt.legend()
    plt.savefig('output/lstm_results.png')
    print("LSTM results saved to output/lstm_results.png")
    plt.show()
else:
    print("Required column 'CPU usage [MHZ]' not found in the dataset.")

print("LSTM model execution completed.")